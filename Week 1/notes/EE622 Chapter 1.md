# Chapter 1: Basics of Transformer Architectures for Biometric Analysis

## Evolution from CNNs and RNNs to Transformers

Early Deep Learning (DL) models for vision and sequence data were dominated by Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), respectively. CNNs excelled at learning spatial hierarchies in images via localized receptive fields and shared filters, becoming the backbone of computer vision tasks like image classification and face recognition. RNNs (and their gated variants like LSTMs) were the go-to for sequential data (e.g., speech or text), processing inputs one timestep at a time to capture temporal dependencies. However, both approaches had limitations. RNNs struggled with long-range dependencies due to vanishing gradients and could not be easily parallelized, as they processed sequences sequentially. CNNs, while parallelizable, had fixed local receptive fields – capturing global context required deep stacks of layers, and even then, long-range interactions were indirect. Transformer architectures emerged as a solution to these issues: introduced by Vaswani et al. (2017) in *“Attention Is All You Need” \[1\]*, the Transformer did away with recurrence and long convolution pipelines entirely, relying solely on attention mechanisms for sequence processing \[1\]. This innovation enabled significantly more parallelism (processing sequences in one shot rather than step-by-step) and better capture of long-range relationships. Indeed, the original Transformer showed superior accuracy on machine translation compared to RNN-based models while training faster due to its parallelizable design. The success in NLP soon carried over to vision tasks. By 2020, researchers demonstrated that a pure Transformer model could operate on images (by treating image patches as sequence tokens) and outperform CNNs on large-scale image recognition benchmarks \[2\]. This was a pivotal moment: it indicated that the inductive biases of CNNs (locality and translational weight sharing) were not the only path to understanding images – given enough data and computation, self-attention could learn global and local image features effectively. In summary, the field evolved from CNNs and RNNs to Transformers to overcome the bottlenecks of locality and sequential processing. Transformers brought a paradigm shift by modeling pairwise interactions between all inputs through attention, allowing models to dynamically learn what to focus on irrespective of distance in space or time.

## Key Innovations in Transformer Architecture

The Transformer architecture introduced several key innovations that enabled its success, especially relevant to biometric data analysis. We highlight three fundamental components: self-attention mechanisms, positional encoding, and the encoder–decoder architecture (particularly in sequence transduction contexts).

### Self-Attention Mechanism

At the heart of every Transformer block is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input relative to each other. Instead of a fixed geometric neighborhood (as in CNN filters), each element (token) in the input can potentially attend to any other element. This is implemented by computing pairwise attention weights using query, key, and value vectors derived from the inputs. For a set of input tokens (represented as vectors), the Transformer computes attention as:

![][image1]

where Q, K, V are matrices collecting the *query*, *key*, and *value* vectors for all tokens, and dk​​ is the dimensionality of queries/keys. The softmax term produces an attention matrix whose entries indicate how strongly each token (row) attends to another token (column). This allows the model to dynamically focus on salient features: for example, in a face image, a token encoding an eye region could attend to another token encoding the other eye or surrounding skin texture if those relationships are important for recognition. Multi-head self-attention extends this idea by having multiple attention heads in parallel, each with its own learned projection (different Q,K,V matrices). Each head can learn to attend to different patterns or features (one head might focus on global structure while another on fine details). The outputs of all heads are then combined. This multi-head approach greatly increases the model’s expressiveness, as different heads can capture complementary aspects of the biometric pattern. In biometric tasks, this means one attention head could learn to focus on minutiae-rich regions (like ridge endings in a fingerprint or distinguishing marks on a face), while another head captures broader context (like the overall ridge flow or face shape). The self-attention mechanism thus provides dynamic feature routing: depending on the input content, information from one part of the biometric can be routed to influence another part adaptively. This is in contrast to CNNs, which always aggregate information from a fixed local neighborhood – the Transformer can draw long-range connections (e.g., relating two distant minutiae points in a fingerprint) in a single layer. As a result, self-attention is especially powerful for biometric traits that have complex global patterns (like the network of blood vessels in a retinal scan or the full topology of fingerprint ridges) because it can integrate evidence across the entire input.

### Positional Encodings

One challenge with using self-attention on sequences (or image patch sequences) is that the mechanism itself is invariant to position – if we permute the tokens, the set of QK^T dot-products remains the same. Traditional sequences (text, speech) and images have meaningful ordering and spatial structure. Transformers handle this by adding a positional encoding to each token’s representation. Positional encodings can be learned vectors or fixed functions (the original Transformer used a sinusoidal positional encoding). The sinusoidal encoding for position pos and feature index i is defined as:

![][image2]

ensuring each position has a unique, smooth embedding that the model can use to infer relative positions. In images, a 2D positional encoding (often separable into x and y components) is used when flattening patches. Positional information is crucial for biometrics: consider a fingerprint – the spatial arrangement of ridges and minutiae defines identity. If a model were oblivious to positions, it might mix up features from disparate regions. With positional encodings, a Transformer knows, for instance, that Token \#5 corresponds to, say, the top-left region of an iris image, whereas Token \#50 might be the bottom-right. This allows the self-attention to respect and utilize spatial layout (e.g., attending to adjacent ridge segments in order).

### Encoder–Decoder Architecture

The original Transformer introduced an encoder–decoder architecture for sequence-to-sequence tasks like translation. The encoder is a stack of self-attention layers that transforms an input sequence (e.g. a biometric data sequence or image patches) into a set of contextualized embeddings. The decoder is another stack that takes the encoder’s output and generates an output sequence (often using *cross-attention* to attend to encoder embeddings, and self-attention for the output sequence being generated). In many vision applications (like image classification or biometric feature extraction), a full encoder–decoder may not be needed – often just an encoder suffices to produce a rich representation of the input. For example, Vision Transformers for image recognition use an encoder that outputs a representation (with a special *\[CLS\]* token for classification). However, the encoder–decoder idea becomes relevant in certain biometric scenarios. For instance, consider cross-modal biometric matching (such as matching an infrared vein pattern to a visible-light image of the hand) – one could use an encoder–decoder where the encoder processes one modality and the decoder (with cross-attention) tries to reconstruct or map to the other modality. Indeed, Transformers have been applied in multi-modal biometric verification where cross-attention helps relate information from, say, a thermal face image to a visible face image of the same person. In summary, the encoder–decoder structure provides flexibility for tasks that involve transducing one representation to another, which can be useful for biometric data integration (e.g., fusing face and voice biometrics, or enhancing a partial fingerprint by “translating” it into a complete one using learned patterns).

## How Self-Attention Benefits Biometric Trait Analysis

Biometric traits – such as fingerprints, iris patterns, facial features, or vein patterns – often contain complex structures and long-range dependencies. The self-attention mechanism directly addresses some of the unique challenges in biometric analysis:

* Long-Range Dependency Modeling: Biometric patterns are not entirely local. For example, the relative configuration of facial features (eyes, nose, mouth) or the pattern formed by distant minutiae points in a fingerprint can be crucial for identity. Self-attention naturally captures such long-range relationships in one layer by allowing any token to attend to any other \[3\]. This means a Transformer analyzing a palm vein image can correlate distant vein branches, or a Transformer analyzing a face can relate information from a scar on the forehead to the shape of the jaw, if that’s pertinent to identification.

* Dynamic Focus on Relevant Features: Different biometric samples, even of the same type, may require focusing on different features. For instance, one fingerprint might have very clear minutiae in one region while another has clearer ridge texture in another region. Transformers can dynamically shift their attention to whatever features are most discriminatory in a given sample. In contrast, a CNN with fixed filters might always emphasize certain frequencies or shapes, potentially overlooking idiosyncratic features that matter in a particular case. Self-attention provides a content-dependent weighting of features – e.g., if a certain region of a fingerprint is smudged or occluded, the model can down-weight that region and concentrate on another part of the print that is clearer.

* Multi-Modal and Cross-Spectral Integration: Biometric systems increasingly fuse information from multiple sources (e.g., face \+ voice, or combining infrared and visible images). Transformers’ attention mechanism can operate across mixed token sequences or through cross-attention in a multi-stream architecture, enabling integration of heterogeneous biometric features. The ability to capture inter-dependencies between different modalities has been demonstrated in cross-spectral transformers for biometrics \[4\]. For example, a cross-spectral biometric Transformer can learn the relationships between patterns in an infrared vein image and a visible-light periocular (eye region) image, effectively learning a joint representation that links the two. This is important because different biometric modalities may compensate for each other’s weaknesses (vein patterns are robust to spoofing and occlusions like masks, while periocular images carry complementary identity info).

* Robustness to Partially Missing Data: Biometric data can be incomplete – a partial fingerprint, a face with occlusion (e.g., mask or sunglasses), etc. Self-attention offers a graceful way to handle missing or corrupt regions: tokens corresponding to missing parts simply won’t attract much attention (they carry little useful information), and the model can still connect the remaining informative parts. There is no strict requirement for a contiguous receptive field to cover the gap, unlike a CNN that might propagate corrupted information through convolution. In essence, attention can skip over missing data by reweighting focus to available data. Later in Chapter 2, we will see how this property is exploited for partial fingerprint recognition.

In summary, self-attention confers Transformers the ability to globally reason over biometric inputs, focusing on salient patterns and ignoring irrelevant noise or missing parts. This aligns well with the needs of biometric analysis, where the spatial arrangement and global coherence of features often determine identity.

## Vision Transformers (ViT) and Their Application to Biometrics

The extension of Transformers to image analysis gave rise to the Vision Transformer (ViT), which has significantly influenced biometric recognition research. ViT was first demonstrated by Dosovitskiy et al. (2020) as a *pure transformer* applied to image patches \[2\]. The model splits an image into fixed-size patches (e.g., 16×16 pixels), flattens each patch into a vector, linearly projects it to an embedding, and feeds the sequence of embeddings into Transformer encoder layers (with an added classification token). This design proved that, given sufficient training data, a ViT can outperform CNNs on image classification benchmarks like ImageNet \[5\]. Notably, ViTs achieved state-of-the-art results when pre-trained on large datasets (e.g., ImageNet-21k) and then fine-tuned to a specific task. This was a critical insight for biometrics because biometric datasets are often relatively small or specialized. By leveraging a ViT pre-trained on generic imagery and fine-tuning it, researchers found they could attain excellent performance on various biometric tasks despite limited biometric data.

In the context of biometric applications, Vision Transformers have been successfully applied across several modalities:

* Facial Recognition: Transformers have been explored to supplement or replace CNN backbones in face recognition systems. While CNNs like ResNet+ArcFace have dominated this area, recent studies use ViT architectures to learn face embeddings, sometimes in a hybrid form (CNN for low-level features, Transformer for high-level aggregation). The benefit is again the global reasoning: a Transformer-based face model can capture interactions between distant facial landmarks or skin patches that a CNN might not link until very deep layers.

* Iris Recognition: An eye’s iris pattern is a textured pattern – some works use Transformers (or local-attention variants) to encode the ring-shaped iris texture, taking advantage of the long-range pattern comparisons that attention enables (for instance, relating two distant iris crypts or furrows).

* Fingerprint and Palmprint Recognition: (We will cover fingerprint in depth in Chapter 2.) Transformers have been used to encode whole fingerprint images into fixed-length representations or to enhance minutiae-based matching. Similarly, palmprints (which have features like principal lines and texture) have been analyzed with ViT-based models.

* Vein Pattern Recognition: ViTs have been very effective in vascular biometrics, such as finger vein, palm vein, or wrist vein recognition. These tasks involve patterns of blood vessels beneath the skin captured via near-infrared imaging. A CNN might pick up local vein segments, but a ViT can better integrate the overall vein network structure. In fact, the first application of pure pre-trained ViTs in vascular biometrics (Garcia-Martin et al., 2023\) achieved exceptionally high identification rates across finger, palm, dorsal hand, and wrist vein datasets, after fine-tuning from ImageNet. For example, on a finger vein dataset (HKPU), the ViT-based approach reached \~99.5% True Positive Identification Rate, and similarly high performance (\~99%) was observed on a variety of vein datasets. These results surpassed prior CNN-based methods, illustrating the viability of Transformers in even small-sample biometric domains when transfer learning is used.

One reason ViTs excel in biometrics is their capacity to model high-level feature interactions without inductive bias. CNNs have a built-in bias for local translational features, which is useful but can also miss global patterns (like the overall topology of veins or ridges). Transformers, by contrast, treat the image more like a graph of patches where any patch can influence any other. This can uncover subtle global patterns important for identity. Additionally, ViTs can be more parameter-efficient in some cases – for a given level of accuracy, a ViT may require fewer multiply-add operations than an equivalently performing CNN (though ViTs usually need more data to train from scratch). The ability to train ViTs on generic data and fine-tune means practitioners can deploy Transformers in biometrics without always requiring massive biometric datasets – a significant practical advantage.

## Case Study: Vein Pattern Recognition with Transformers

Vein pattern recognition is an emerging biometric modality that benefits greatly from Transformer architectures. Vein biometrics (e.g., scanning the blood vessel patterns in one’s finger, hand, or forehead) are attractive because they are hidden (internal) traits – difficult to counterfeit and unaffected by external obscuration like gloves or cosmetics. However, vein images often have complex patterns of branching vessels that span the entire image. Traditional CNN approaches can detect local vessel segments but may miss the forest for the trees if the vessel network is discontinuous or partially visible. Transformers, with global self-attention, are well-suited to capture the connectivity and overall layout of vein networks.

A notable example is the work by Garcia-Martin and Sanchez-Reillo (2023), who applied Vision Transformers to Vascular Biometric Recognition (VBR). They fine-tuned pre-trained ViT models on fourteen vein datasets encompassing finger vein, palm vein, dorsal hand vein, and wrist vein images. The ViTs achieved extremely high identification accuracies (in many cases \>99% TPIR) across these datasets. These results not only outperformed many CNN-based benchmarks but also demonstrated the versatility of a single Transformer architecture across different vein modalities. The key advantage observed was the transfer learning capability: by leveraging features learned from general computer vision data, the ViT could recognize vein patterns even with the limited samples typical in each vein dataset.

Beyond accuracy, explainability in vein transformers has been explored. In vein recognition, it’s important to ensure the model is focusing on actual vein structures rather than, say, incidental artifacts or lighting differences. Researchers have used attention map visualization (discussed later) to confirm that ViT models focus on the regions with significant vein patterns. For instance, an explainability study reported that recognition performance improves when the ViT’s attention is concentrated on true vein regions, and they visualized attention to show the model highlighting the vein lines on wrist images \[6\]. This not only builds trust in the system (the model is looking at the “right” features) but also provides a measure of security: if the attention map were diffuse or focused on irrelevant regions, it could indicate the model is picking up spurious cues that might fail under presentation attacks or varied conditions.

Another case study is the Forehead Vein and Periocular Transformer by Sharma et al. (2025). In the wake of COVID-19, face recognition faltered due to masks, and fingerprint use declined due to hygiene concerns \[7\]. This prompted exploration of contactless biometrics like forehead subcutaneous veins (captured via IR) combined with periocular (eye region) images. Sharma et al. proposed a *Cross-Spectral Vision Transformer (CS-ViT)* for this task. Their model uses a dual-channel Transformer: one stream for vein images and one for periocular images, with specialized cross-spectral attention modules that connect the two. Each channel’s Transformer uses a Phase-Only correlation-based self-attention to remain robust to illumination and intensity differences between spectra. The CS-ViT achieved 98.8% classification accuracy, combining vein and periocular data – a remarkable performance demonstrating that even lightweight Transformer models can effectively fuse heterogeneous biometric traits. This case highlights how Transformers facilitate multi-biometric fusion: the attention mechanism can learn the optimal way to weight and combine features from two different sources (IR vein vs. visible eye image) for each individual, something that would be harder to achieve with separate models or simple score fusion. Moreover, by being *cross-spectral*, the model handles different imaging conditions within one architecture, showcasing Transformers’ flexibility.

In both vein-focused cases, a theme emerges: Transformers can capture complex relational patterns inherent in biometric traits (like branching vein structures) better than purely local methods. They also easily support multi-input setups (two images, multi-view data, etc.) via multi-head and cross-attention. This makes them an excellent choice for advanced biometric systems that aim to be robust (working under occlusion/mask), multi-modal, and secure.

## Case Study: Cross-Spectral Biometrics (CS-ViT)

*(Covered partially in the previous section as the forehead vein \+ periocular example, we provide additional context here.)*

Cross-spectral biometrics refers to matching or fusing data captured in different spectra or sensing modalities – for example, verifying if an infrared face image matches a visible-light face image, or using thermal hand vein images in conjunction with an RGB photo of a hand. This is challenging because different spectra reveal different aspects of the biometric trait. Traditional systems might require separate feature extractors and a carefully designed fusion algorithm. Transformers simplify this by offering a unified framework where different spectral inputs can be processed in parallel and then integrated through attention.

The CS-ViT by Sharma et al. \[7\] is a prime example. It employs a dual transformer encoder: one for each spectral channel. Crucially, it introduces a specialized attention mechanism called *Phase-Only Correlation Cross-Spectral Attention (POC-CSA)* that links the two channels. Phase-Only Correlation is a technique known in image matching to align images using the phase of their Fourier transform – by incorporating it into the attention, the CS-ViT effectively aligns features from the vein image with those from the periocular image, making the attention robust to resolution or illumination differences between IR and visible images. In practice, one can imagine that the model learns relationships like: “this pattern in the vein image (e.g., a certain bifurcation in the forehead veins) corresponds to that pattern in the periocular region (e.g., a particular wrinkle or brow shape)” which might correlate across people. The cross-spectral attention ensures the combined representation leverages these correlations. The result is a highly discriminative fused representation, as evidenced by near-perfect accuracy on a test database.

From a security perspective, cross-spectral transformers like CS-ViT add resilience against spoofing or environmental challenges. A face mask might block many visible facial features, but an IR vein camera can still capture the unique vein pattern. A combined transformer will recognize the person from the vein pattern even if the rest of the face is covered. Similarly, if lighting is poor for a normal camera, an IR-based trait could fill in. The attention mechanism’s flexibility means the model can lean more on whatever modality is more informative for a given subject or condition. This adaptive multi-modal feature usage is another strength of Transformers that static fusion methods lack.

In summary, cross-spectral biometric Transformers represent a cutting-edge application where Transformer architectures unify heterogeneous biometric sources. They exemplify the broader trend in biometrics to move beyond single-modality recognition toward integrated systems that use whatever biometric evidence is available, exactly the kind of problem Transformers are inherently designed to tackle (integrating information from multiple signals through learned attention).

## Visualizing Attention Maps for Interpretability and Security

Transformers offer not only powerful performance but also new ways to interpret model decisions. Each Transformer layer produces attention weight matrices that show how the model is attending to different parts of the input. By visualizing these attention maps, we can gain insight into what the model “thinks” is important. This is particularly valuable in biometrics for both interpretability (to verify the model uses genuine biometric features) and security (to ensure the model isn’t focusing on an artifact that could be exploited).

One common approach is to visualize the attention from the classification token (in a ViT) to the image patch tokens. For an image input, one can take the final layer’s attention matrix for the class token and reshape it to the spatial layout of patches. The result is essentially a heatmap over the image, highlighting regions that contributed most to the model’s decision. There are also techniques like Attention Rollout, which accumulates attention weights across all layers to get a more holistic importance map. The idea from Abnar & Zuidema (2020) is to treat the attention matrices as directed connectivity and multiply them through the layers (with appropriate normalization and identity skip addition) to obtain an overall influence map. This rolled-out attention can then be visualized. In practice, this means we can produce an image where, for example, the forehead and eyes light up for a face recognition transformer (meaning those areas were most influential), or the core and delta regions light up for a fingerprint transformer (indicating the model focused on those global landmarks and minutiae around them).

Figure 1 shows an example of attention visualization on incomplete fingerprint images. In this example, the model (Finger Recovery Transformer, see Chapter 2\) is dealing with partial or noisy fingerprint inputs. The attention maps (overlaid in red intensity) indicate which parts of the image the transformer is focusing on to reconstruct and recognize the fingerprint. We can see that even when large portions of the fingerprint are missing or smudged, the model concentrates on the remaining clear ridge flows and minutiae-dense areas, effectively ignoring the gaps or noise. This aligns with how a forensic examiner might focus on any available minutiae in a partial print. Such visualizations confirm that the transformer isn’t guessing blindly; it’s leveraging the structured information that remains.

*Fig. 1: Examples of partial fingerprint inputs (left: rolled print with missing area; middle: a “snapped” finger with only tip captured; right: a latent fingerprint with noisy background) and the attention map learned by a Transformer-based model. The red overlay highlights regions with high attention – the model focuses on the available ridge structures and minutiae while disregarding areas of missing or irrelevant data. This interpretability check ensures the model’s decisions are based on legitimate fingerprint features, enhancing trust and security.*

Visualizing attention is not limited to class tokens. We can also inspect intermediate attention between patches. For instance, one could pick a particular patch (say a region of a vein image) and see which other patches it attends to strongly – this might reveal that a certain vein bifurcation patch attends strongly to another patch further along the same vein, indicating the model has linked those two parts of the vein as part of one structure. This kind of interpretability is valuable for debugging models (are they looking at the finger’s pattern or the background? are they focusing on a scar that might not be stable as an identifier?) and for user confidence (presenting a user with a heatmap on their face showing the system focused on their eyes and not, say, an irrelevant backdrop).

From a security standpoint, attention maps can help detect if a model is focusing on potentially spoofable cues. Suppose a face transformer consistently paid high attention to the border of the image; that might indicate it’s picking up on some sensor-specific artifact rather than facial features, which could be a vulnerability. By identifying such issues via attention visualization, developers can retrain the model or adjust the data to refocus attention on proper biometric traits.

It’s important to note that attention visualization is an explanatory tool, but not a complete explanation of a model’s decision. There is debate in the literature whether attention weights directly correlate with feature importance. Nonetheless, in practice, they often provide a useful approximation. Techniques like Grad-CAM (from CNNs) have analogues in transformers as well, and hybrid methods are being researched \[8\]. Still, the direct availability of attention matrices in Transformers makes them one of the more transparent deep learning models for analysis.

In conclusion, attention map visualization serves as a bridge between the complex computations of a transformer and human understanding. For biometric systems that operate in high-stakes domains (security checkpoints, forensic analysis), having the ability to explain why the model matched a fingerprint or flagged a face is crucial. Transformers offer this to a greater extent than previous biometric models, thereby not only improving performance but also enabling interpretable and trustworthy biometric AI systems.

## Summary

Transformers have revolutionized the landscape of deep learning, and their impact is increasingly felt in biometric analysis. In this chapter, we reviewed how the field progressed from CNNs and RNNs towards transformers to overcome limitations in modeling long-range dependencies and parallelizing computation. We examined the core innovations of the transformer architecture – particularly self-attention and positional encodings – and why they are a natural fit for biometric data, which often contains globally dependent features (like patterns spread across a fingerprint or face). Vision Transformers have matched or surpassed CNN performance in many vision tasks, and biometrics is no exception: we discussed case studies in vein pattern recognition and cross-spectral biometrics where transformers achieved state-of-the-art results by effectively fusing information and capturing complex feature relationships. A key advantage of transformers in these applications is their flexibility – a single architecture can integrate multiple modalities and focus on the most salient traits of a biometric identifier, yielding robust performance even under occlusions or spectrum changes. Finally, we highlighted how attention maps from transformers can be visualized to interpret model decisions, giving insight into what parts of a biometric trait the model relies on. This interpretability not only builds confidence in automated systems but can enhance security by ensuring models use genuine biometric features rather than spurious cues. In summary, transformers provide a powerful, unifying framework for biometric analysis, offering accuracy, adaptability, and transparency. The next chapter will delve deeper into a specific application: using transformers for fingerprint feature extraction, where we will see these concepts applied to one of the oldest and most studied biometric modalities.

---

[image1]: <data:image/png;base64,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>

[image2]: <data:image/png;base64,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>